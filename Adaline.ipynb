{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adaline:\n",
    "\n",
    "    def __init__(self, learning_rate=0.01, max_iter=1000, random_state=42, batch_size=64, tol=1e-6) -> None:\n",
    "        \n",
    "        np.random.seed(random_state)\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.tol = tol\n",
    "        self.coefs_ = None\n",
    "    \n",
    "\n",
    "    def batch_generator(self, X, y):\n",
    "\n",
    "        m, _ = X.shape\n",
    "        indices = np.arange(m)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        for begin in np.arange(0, m, self.batch_size):\n",
    "\n",
    "            batch_indice = indices[begin : min(begin + self.batch_size, m)]\n",
    "            yield X[batch_indice], y[batch_indice]\n",
    "    \n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        m, _ = X.shape\n",
    "        X = np.hstack((np.ones((m, 1)), X))\n",
    "\n",
    "        return np.where(self.activation(np.dot(X, self.coefs_)) > 0.5, 1, 0)\n",
    "    \n",
    "    def activation(self, z):\n",
    "\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        m, n = X.shape\n",
    "        X = np.hstack((np.ones((m, 1)), X))\n",
    "\n",
    "        if y.ndim == 1:\n",
    "\n",
    "            self.coefs_ = np.random.uniform(-0.5, 0.5, size=(n + 1, ))\n",
    "        \n",
    "        else:\n",
    "\n",
    "            self.coefs_ = np.random.uniform(-0.5, 0.5, size=(n + 1, 1))\n",
    "        \n",
    "\n",
    "        for iteration in range(self.max_iter):\n",
    "\n",
    "            full_batch_loss = 0\n",
    "\n",
    "            for x_batch, y_batch in self.batch_generator(X, y):\n",
    "\n",
    "                pred = self.activation(np.dot(x_batch, self.coefs_))\n",
    "                full_batch_loss += -np.mean(y_batch * np.log(pred) + (1 - y_batch) * np.log(1 - pred))\n",
    "    \n",
    "                self.coefs_ -= self.learning_rate * 1 / x_batch.shape[0] * np.dot(x_batch.T, pred - y_batch)\n",
    "\n",
    "            full_batch_loss /= X.shape[0] // self.batch_size\n",
    "            print(f\"Iteration {iteration + 1}/{self.max_iter} - Loss: {full_batch_loss:.6f}\")\n",
    "\n",
    "            if full_batch_loss < self.tol:\n",
    "\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/1000 - Loss: 0.721232\n",
      "Iteration 2/1000 - Loss: 0.720512\n",
      "Iteration 3/1000 - Loss: 0.702636\n",
      "Iteration 4/1000 - Loss: 0.683401\n",
      "Iteration 5/1000 - Loss: 0.669957\n",
      "Iteration 6/1000 - Loss: 0.694250\n",
      "Iteration 7/1000 - Loss: 0.665629\n",
      "Iteration 8/1000 - Loss: 0.665116\n",
      "Iteration 9/1000 - Loss: 0.647793\n",
      "Iteration 10/1000 - Loss: 0.626119\n",
      "Iteration 11/1000 - Loss: 0.638959\n",
      "Iteration 12/1000 - Loss: 0.658991\n",
      "Iteration 13/1000 - Loss: 0.611521\n",
      "Iteration 14/1000 - Loss: 0.647651\n",
      "Iteration 15/1000 - Loss: 0.583456\n",
      "Iteration 16/1000 - Loss: 0.604409\n",
      "Iteration 17/1000 - Loss: 0.601156\n",
      "Iteration 18/1000 - Loss: 0.607544\n",
      "Iteration 19/1000 - Loss: 0.571478\n",
      "Iteration 20/1000 - Loss: 0.576417\n",
      "Iteration 21/1000 - Loss: 0.560621\n",
      "Iteration 22/1000 - Loss: 0.541612\n",
      "Iteration 23/1000 - Loss: 0.547654\n",
      "Iteration 24/1000 - Loss: 0.568710\n",
      "Iteration 25/1000 - Loss: 0.541384\n",
      "Iteration 26/1000 - Loss: 0.524935\n",
      "Iteration 27/1000 - Loss: 0.567611\n",
      "Iteration 28/1000 - Loss: 0.495091\n",
      "Iteration 29/1000 - Loss: 0.512043\n",
      "Iteration 30/1000 - Loss: 0.499730\n",
      "Iteration 31/1000 - Loss: 0.507532\n",
      "Iteration 32/1000 - Loss: 0.510697\n",
      "Iteration 33/1000 - Loss: 0.503796\n",
      "Iteration 34/1000 - Loss: 0.496742\n",
      "Iteration 35/1000 - Loss: 0.508784\n",
      "Iteration 36/1000 - Loss: 0.486219\n",
      "Iteration 37/1000 - Loss: 0.465029\n",
      "Iteration 38/1000 - Loss: 0.517698\n",
      "Iteration 39/1000 - Loss: 0.460134\n",
      "Iteration 40/1000 - Loss: 0.482137\n",
      "Iteration 41/1000 - Loss: 0.467684\n",
      "Iteration 42/1000 - Loss: 0.470424\n",
      "Iteration 43/1000 - Loss: 0.443258\n",
      "Iteration 44/1000 - Loss: 0.458206\n",
      "Iteration 45/1000 - Loss: 0.438603\n",
      "Iteration 46/1000 - Loss: 0.442447\n",
      "Iteration 47/1000 - Loss: 0.453227\n",
      "Iteration 48/1000 - Loss: 0.443055\n",
      "Iteration 49/1000 - Loss: 0.462289\n",
      "Iteration 50/1000 - Loss: 0.425917\n",
      "Iteration 51/1000 - Loss: 0.452502\n",
      "Iteration 52/1000 - Loss: 0.430528\n",
      "Iteration 53/1000 - Loss: 0.407565\n",
      "Iteration 54/1000 - Loss: 0.440974\n",
      "Iteration 55/1000 - Loss: 0.418966\n",
      "Iteration 56/1000 - Loss: 0.421876\n",
      "Iteration 57/1000 - Loss: 0.422057\n",
      "Iteration 58/1000 - Loss: 0.406448\n",
      "Iteration 59/1000 - Loss: 0.402334\n",
      "Iteration 60/1000 - Loss: 0.410806\n",
      "Iteration 61/1000 - Loss: 0.393202\n",
      "Iteration 62/1000 - Loss: 0.398843\n",
      "Iteration 63/1000 - Loss: 0.402377\n",
      "Iteration 64/1000 - Loss: 0.385298\n",
      "Iteration 65/1000 - Loss: 0.424991\n",
      "Iteration 66/1000 - Loss: 0.396400\n",
      "Iteration 67/1000 - Loss: 0.394118\n",
      "Iteration 68/1000 - Loss: 0.398571\n",
      "Iteration 69/1000 - Loss: 0.371483\n",
      "Iteration 70/1000 - Loss: 0.399343\n",
      "Iteration 71/1000 - Loss: 0.370647\n",
      "Iteration 72/1000 - Loss: 0.360051\n",
      "Iteration 73/1000 - Loss: 0.378105\n",
      "Iteration 74/1000 - Loss: 0.384521\n",
      "Iteration 75/1000 - Loss: 0.409329\n",
      "Iteration 76/1000 - Loss: 0.378449\n",
      "Iteration 77/1000 - Loss: 0.382103\n",
      "Iteration 78/1000 - Loss: 0.343919\n",
      "Iteration 79/1000 - Loss: 0.357564\n",
      "Iteration 80/1000 - Loss: 0.396593\n",
      "Iteration 81/1000 - Loss: 0.388319\n",
      "Iteration 82/1000 - Loss: 0.340569\n",
      "Iteration 83/1000 - Loss: 0.353128\n",
      "Iteration 84/1000 - Loss: 0.343000\n",
      "Iteration 85/1000 - Loss: 0.343936\n",
      "Iteration 86/1000 - Loss: 0.358618\n",
      "Iteration 87/1000 - Loss: 0.343179\n",
      "Iteration 88/1000 - Loss: 0.349385\n",
      "Iteration 89/1000 - Loss: 0.339260\n",
      "Iteration 90/1000 - Loss: 0.385148\n",
      "Iteration 91/1000 - Loss: 0.340299\n",
      "Iteration 92/1000 - Loss: 0.336754\n",
      "Iteration 93/1000 - Loss: 0.326969\n",
      "Iteration 94/1000 - Loss: 0.338014\n",
      "Iteration 95/1000 - Loss: 0.335143\n",
      "Iteration 96/1000 - Loss: 0.338979\n",
      "Iteration 97/1000 - Loss: 0.362588\n",
      "Iteration 98/1000 - Loss: 0.321203\n",
      "Iteration 99/1000 - Loss: 0.331335\n",
      "Iteration 100/1000 - Loss: 0.349352\n",
      "Iteration 101/1000 - Loss: 0.322337\n",
      "Iteration 102/1000 - Loss: 0.344297\n",
      "Iteration 103/1000 - Loss: 0.330242\n",
      "Iteration 104/1000 - Loss: 0.330985\n",
      "Iteration 105/1000 - Loss: 0.347705\n",
      "Iteration 106/1000 - Loss: 0.336999\n",
      "Iteration 107/1000 - Loss: 0.321371\n",
      "Iteration 108/1000 - Loss: 0.323030\n",
      "Iteration 109/1000 - Loss: 0.324974\n",
      "Iteration 110/1000 - Loss: 0.297827\n",
      "Iteration 111/1000 - Loss: 0.308536\n",
      "Iteration 112/1000 - Loss: 0.306307\n",
      "Iteration 113/1000 - Loss: 0.319880\n",
      "Iteration 114/1000 - Loss: 0.301839\n",
      "Iteration 115/1000 - Loss: 0.312769\n",
      "Iteration 116/1000 - Loss: 0.304339\n",
      "Iteration 117/1000 - Loss: 0.314137\n",
      "Iteration 118/1000 - Loss: 0.324912\n",
      "Iteration 119/1000 - Loss: 0.295454\n",
      "Iteration 120/1000 - Loss: 0.308361\n",
      "Iteration 121/1000 - Loss: 0.280815\n",
      "Iteration 122/1000 - Loss: 0.300664\n",
      "Iteration 123/1000 - Loss: 0.302582\n",
      "Iteration 124/1000 - Loss: 0.286665\n",
      "Iteration 125/1000 - Loss: 0.326356\n",
      "Iteration 126/1000 - Loss: 0.295362\n",
      "Iteration 127/1000 - Loss: 0.290234\n",
      "Iteration 128/1000 - Loss: 0.293863\n",
      "Iteration 129/1000 - Loss: 0.291920\n",
      "Iteration 130/1000 - Loss: 0.296213\n",
      "Iteration 131/1000 - Loss: 0.279546\n",
      "Iteration 132/1000 - Loss: 0.284705\n",
      "Iteration 133/1000 - Loss: 0.280078\n",
      "Iteration 134/1000 - Loss: 0.321726\n",
      "Iteration 135/1000 - Loss: 0.273196\n",
      "Iteration 136/1000 - Loss: 0.294346\n",
      "Iteration 137/1000 - Loss: 0.291364\n",
      "Iteration 138/1000 - Loss: 0.271318\n",
      "Iteration 139/1000 - Loss: 0.289090\n",
      "Iteration 140/1000 - Loss: 0.290619\n",
      "Iteration 141/1000 - Loss: 0.279985\n",
      "Iteration 142/1000 - Loss: 0.286323\n",
      "Iteration 143/1000 - Loss: 0.286919\n",
      "Iteration 144/1000 - Loss: 0.287892\n",
      "Iteration 145/1000 - Loss: 0.267997\n",
      "Iteration 146/1000 - Loss: 0.282640\n",
      "Iteration 147/1000 - Loss: 0.283819\n",
      "Iteration 148/1000 - Loss: 0.261607\n",
      "Iteration 149/1000 - Loss: 0.262312\n",
      "Iteration 150/1000 - Loss: 0.307629\n",
      "Iteration 151/1000 - Loss: 0.280228\n",
      "Iteration 152/1000 - Loss: 0.267222\n",
      "Iteration 153/1000 - Loss: 0.272051\n",
      "Iteration 154/1000 - Loss: 0.279611\n",
      "Iteration 155/1000 - Loss: 0.284965\n",
      "Iteration 156/1000 - Loss: 0.284283\n",
      "Iteration 157/1000 - Loss: 0.262579\n",
      "Iteration 158/1000 - Loss: 0.310945\n",
      "Iteration 159/1000 - Loss: 0.263397\n",
      "Iteration 160/1000 - Loss: 0.260029\n",
      "Iteration 161/1000 - Loss: 0.278364\n",
      "Iteration 162/1000 - Loss: 0.253697\n",
      "Iteration 163/1000 - Loss: 0.254184\n",
      "Iteration 164/1000 - Loss: 0.251711\n",
      "Iteration 165/1000 - Loss: 0.284777\n",
      "Iteration 166/1000 - Loss: 0.250672\n",
      "Iteration 167/1000 - Loss: 0.265885\n",
      "Iteration 168/1000 - Loss: 0.247231\n",
      "Iteration 169/1000 - Loss: 0.268449\n",
      "Iteration 170/1000 - Loss: 0.279544\n",
      "Iteration 171/1000 - Loss: 0.258304\n",
      "Iteration 172/1000 - Loss: 0.268272\n",
      "Iteration 173/1000 - Loss: 0.252212\n",
      "Iteration 174/1000 - Loss: 0.236046\n",
      "Iteration 175/1000 - Loss: 0.268715\n",
      "Iteration 176/1000 - Loss: 0.252371\n",
      "Iteration 177/1000 - Loss: 0.244662\n",
      "Iteration 178/1000 - Loss: 0.251438\n",
      "Iteration 179/1000 - Loss: 0.265533\n",
      "Iteration 180/1000 - Loss: 0.254777\n",
      "Iteration 181/1000 - Loss: 0.254974\n",
      "Iteration 182/1000 - Loss: 0.270577\n",
      "Iteration 183/1000 - Loss: 0.245784\n",
      "Iteration 184/1000 - Loss: 0.253563\n",
      "Iteration 185/1000 - Loss: 0.244641\n",
      "Iteration 186/1000 - Loss: 0.230987\n",
      "Iteration 187/1000 - Loss: 0.240384\n",
      "Iteration 188/1000 - Loss: 0.235082\n",
      "Iteration 189/1000 - Loss: 0.257063\n",
      "Iteration 190/1000 - Loss: 0.245543\n",
      "Iteration 191/1000 - Loss: 0.247398\n",
      "Iteration 192/1000 - Loss: 0.228019\n",
      "Iteration 193/1000 - Loss: 0.239946\n",
      "Iteration 194/1000 - Loss: 0.251788\n",
      "Iteration 195/1000 - Loss: 0.248649\n",
      "Iteration 196/1000 - Loss: 0.242629\n",
      "Iteration 197/1000 - Loss: 0.230007\n",
      "Iteration 198/1000 - Loss: 0.240591\n",
      "Iteration 199/1000 - Loss: 0.240555\n",
      "Iteration 200/1000 - Loss: 0.217707\n",
      "Iteration 201/1000 - Loss: 0.224779\n",
      "Iteration 202/1000 - Loss: 0.219884\n",
      "Iteration 203/1000 - Loss: 0.227121\n",
      "Iteration 204/1000 - Loss: 0.276699\n",
      "Iteration 205/1000 - Loss: 0.234650\n",
      "Iteration 206/1000 - Loss: 0.271880\n",
      "Iteration 207/1000 - Loss: 0.225579\n",
      "Iteration 208/1000 - Loss: 0.223158\n",
      "Iteration 209/1000 - Loss: 0.236598\n",
      "Iteration 210/1000 - Loss: 0.290833\n",
      "Iteration 211/1000 - Loss: 0.226099\n",
      "Iteration 212/1000 - Loss: 0.231503\n",
      "Iteration 213/1000 - Loss: 0.230693\n",
      "Iteration 214/1000 - Loss: 0.232659\n",
      "Iteration 215/1000 - Loss: 0.247057\n",
      "Iteration 216/1000 - Loss: 0.242147\n",
      "Iteration 217/1000 - Loss: 0.230528\n",
      "Iteration 218/1000 - Loss: 0.248439\n",
      "Iteration 219/1000 - Loss: 0.220687\n",
      "Iteration 220/1000 - Loss: 0.253577\n",
      "Iteration 221/1000 - Loss: 0.231687\n",
      "Iteration 222/1000 - Loss: 0.263600\n",
      "Iteration 223/1000 - Loss: 0.232173\n",
      "Iteration 224/1000 - Loss: 0.212018\n",
      "Iteration 225/1000 - Loss: 0.219385\n",
      "Iteration 226/1000 - Loss: 0.256212\n",
      "Iteration 227/1000 - Loss: 0.234294\n",
      "Iteration 228/1000 - Loss: 0.220039\n",
      "Iteration 229/1000 - Loss: 0.245207\n",
      "Iteration 230/1000 - Loss: 0.219199\n",
      "Iteration 231/1000 - Loss: 0.230562\n",
      "Iteration 232/1000 - Loss: 0.205730\n",
      "Iteration 233/1000 - Loss: 0.209225\n",
      "Iteration 234/1000 - Loss: 0.210570\n",
      "Iteration 235/1000 - Loss: 0.216030\n",
      "Iteration 236/1000 - Loss: 0.234855\n",
      "Iteration 237/1000 - Loss: 0.239261\n",
      "Iteration 238/1000 - Loss: 0.211508\n",
      "Iteration 239/1000 - Loss: 0.236239\n",
      "Iteration 240/1000 - Loss: 0.211102\n",
      "Iteration 241/1000 - Loss: 0.206326\n",
      "Iteration 242/1000 - Loss: 0.222775\n",
      "Iteration 243/1000 - Loss: 0.208984\n",
      "Iteration 244/1000 - Loss: 0.218906\n",
      "Iteration 245/1000 - Loss: 0.214310\n",
      "Iteration 246/1000 - Loss: 0.210137\n",
      "Iteration 247/1000 - Loss: 0.211221\n",
      "Iteration 248/1000 - Loss: 0.223295\n",
      "Iteration 249/1000 - Loss: 0.214269\n",
      "Iteration 250/1000 - Loss: 0.206699\n",
      "Iteration 251/1000 - Loss: 0.204604\n",
      "Iteration 252/1000 - Loss: 0.215092\n",
      "Iteration 253/1000 - Loss: 0.232765\n",
      "Iteration 254/1000 - Loss: 0.213735\n",
      "Iteration 255/1000 - Loss: 0.210222\n",
      "Iteration 256/1000 - Loss: 0.239023\n",
      "Iteration 257/1000 - Loss: 0.198360\n",
      "Iteration 258/1000 - Loss: 0.192411\n",
      "Iteration 259/1000 - Loss: 0.191467\n",
      "Iteration 260/1000 - Loss: 0.243462\n",
      "Iteration 261/1000 - Loss: 0.212390\n",
      "Iteration 262/1000 - Loss: 0.192608\n",
      "Iteration 263/1000 - Loss: 0.216196\n",
      "Iteration 264/1000 - Loss: 0.193486\n",
      "Iteration 265/1000 - Loss: 0.195422\n",
      "Iteration 266/1000 - Loss: 0.199655\n",
      "Iteration 267/1000 - Loss: 0.197919\n",
      "Iteration 268/1000 - Loss: 0.193548\n",
      "Iteration 269/1000 - Loss: 0.234424\n",
      "Iteration 270/1000 - Loss: 0.196267\n",
      "Iteration 271/1000 - Loss: 0.197409\n",
      "Iteration 272/1000 - Loss: 0.197607\n",
      "Iteration 273/1000 - Loss: 0.203163\n",
      "Iteration 274/1000 - Loss: 0.228477\n",
      "Iteration 275/1000 - Loss: 0.193710\n",
      "Iteration 276/1000 - Loss: 0.194688\n",
      "Iteration 277/1000 - Loss: 0.199434\n",
      "Iteration 278/1000 - Loss: 0.205396\n",
      "Iteration 279/1000 - Loss: 0.187922\n",
      "Iteration 280/1000 - Loss: 0.190493\n",
      "Iteration 281/1000 - Loss: 0.224354\n",
      "Iteration 282/1000 - Loss: 0.179847\n",
      "Iteration 283/1000 - Loss: 0.221808\n",
      "Iteration 284/1000 - Loss: 0.215191\n",
      "Iteration 285/1000 - Loss: 0.198524\n",
      "Iteration 286/1000 - Loss: 0.187817\n",
      "Iteration 287/1000 - Loss: 0.178502\n",
      "Iteration 288/1000 - Loss: 0.205512\n",
      "Iteration 289/1000 - Loss: 0.192596\n",
      "Iteration 290/1000 - Loss: 0.201424\n",
      "Iteration 291/1000 - Loss: 0.202275\n",
      "Iteration 292/1000 - Loss: 0.188078\n",
      "Iteration 293/1000 - Loss: 0.201412\n",
      "Iteration 294/1000 - Loss: 0.177937\n",
      "Iteration 295/1000 - Loss: 0.187076\n",
      "Iteration 296/1000 - Loss: 0.184384\n",
      "Iteration 297/1000 - Loss: 0.184081\n",
      "Iteration 298/1000 - Loss: 0.187389\n",
      "Iteration 299/1000 - Loss: 0.178017\n",
      "Iteration 300/1000 - Loss: 0.193890\n",
      "Iteration 301/1000 - Loss: 0.208987\n",
      "Iteration 302/1000 - Loss: 0.172077\n",
      "Iteration 303/1000 - Loss: 0.175759\n",
      "Iteration 304/1000 - Loss: 0.193498\n",
      "Iteration 305/1000 - Loss: 0.177716\n",
      "Iteration 306/1000 - Loss: 0.180093\n",
      "Iteration 307/1000 - Loss: 0.190807\n",
      "Iteration 308/1000 - Loss: 0.188039\n",
      "Iteration 309/1000 - Loss: 0.183959\n",
      "Iteration 310/1000 - Loss: 0.194878\n",
      "Iteration 311/1000 - Loss: 0.173569\n",
      "Iteration 312/1000 - Loss: 0.174187\n",
      "Iteration 313/1000 - Loss: 0.180908\n",
      "Iteration 314/1000 - Loss: 0.177160\n",
      "Iteration 315/1000 - Loss: 0.166987\n",
      "Iteration 316/1000 - Loss: 0.169233\n",
      "Iteration 317/1000 - Loss: 0.180811\n",
      "Iteration 318/1000 - Loss: 0.180987\n",
      "Iteration 319/1000 - Loss: 0.174204\n",
      "Iteration 320/1000 - Loss: 0.197182\n",
      "Iteration 321/1000 - Loss: 0.180584\n",
      "Iteration 322/1000 - Loss: 0.169694\n",
      "Iteration 323/1000 - Loss: 0.178441\n",
      "Iteration 324/1000 - Loss: 0.181345\n",
      "Iteration 325/1000 - Loss: 0.177775\n",
      "Iteration 326/1000 - Loss: 0.177716\n",
      "Iteration 327/1000 - Loss: 0.173264\n",
      "Iteration 328/1000 - Loss: 0.168832\n",
      "Iteration 329/1000 - Loss: 0.167881\n",
      "Iteration 330/1000 - Loss: 0.203116\n",
      "Iteration 331/1000 - Loss: 0.173867\n",
      "Iteration 332/1000 - Loss: 0.187063\n",
      "Iteration 333/1000 - Loss: 0.167357\n",
      "Iteration 334/1000 - Loss: 0.181876\n",
      "Iteration 335/1000 - Loss: 0.181016\n",
      "Iteration 336/1000 - Loss: 0.198835\n",
      "Iteration 337/1000 - Loss: 0.193940\n",
      "Iteration 338/1000 - Loss: 0.165834\n",
      "Iteration 339/1000 - Loss: 0.162672\n",
      "Iteration 340/1000 - Loss: 0.178820\n",
      "Iteration 341/1000 - Loss: 0.175214\n",
      "Iteration 342/1000 - Loss: 0.168280\n",
      "Iteration 343/1000 - Loss: 0.170879\n",
      "Iteration 344/1000 - Loss: 0.180717\n",
      "Iteration 345/1000 - Loss: 0.184282\n",
      "Iteration 346/1000 - Loss: 0.176942\n",
      "Iteration 347/1000 - Loss: 0.174526\n",
      "Iteration 348/1000 - Loss: 0.172348\n",
      "Iteration 349/1000 - Loss: 0.168124\n",
      "Iteration 350/1000 - Loss: 0.161027\n",
      "Iteration 351/1000 - Loss: 0.164656\n",
      "Iteration 352/1000 - Loss: 0.166443\n",
      "Iteration 353/1000 - Loss: 0.173333\n",
      "Iteration 354/1000 - Loss: 0.165149\n",
      "Iteration 355/1000 - Loss: 0.175299\n",
      "Iteration 356/1000 - Loss: 0.167170\n",
      "Iteration 357/1000 - Loss: 0.172386\n",
      "Iteration 358/1000 - Loss: 0.178051\n",
      "Iteration 359/1000 - Loss: 0.177309\n",
      "Iteration 360/1000 - Loss: 0.158705\n",
      "Iteration 361/1000 - Loss: 0.172490\n",
      "Iteration 362/1000 - Loss: 0.158214\n",
      "Iteration 363/1000 - Loss: 0.159733\n",
      "Iteration 364/1000 - Loss: 0.154587\n",
      "Iteration 365/1000 - Loss: 0.173001\n",
      "Iteration 366/1000 - Loss: 0.171762\n",
      "Iteration 367/1000 - Loss: 0.155308\n",
      "Iteration 368/1000 - Loss: 0.180426\n",
      "Iteration 369/1000 - Loss: 0.180771\n",
      "Iteration 370/1000 - Loss: 0.153018\n",
      "Iteration 371/1000 - Loss: 0.157246\n",
      "Iteration 372/1000 - Loss: 0.176955\n",
      "Iteration 373/1000 - Loss: 0.175564\n",
      "Iteration 374/1000 - Loss: 0.172108\n",
      "Iteration 375/1000 - Loss: 0.160775\n",
      "Iteration 376/1000 - Loss: 0.164139\n",
      "Iteration 377/1000 - Loss: 0.172299\n",
      "Iteration 378/1000 - Loss: 0.155199\n",
      "Iteration 379/1000 - Loss: 0.198168\n",
      "Iteration 380/1000 - Loss: 0.192229\n",
      "Iteration 381/1000 - Loss: 0.161748\n",
      "Iteration 382/1000 - Loss: 0.157886\n",
      "Iteration 383/1000 - Loss: 0.169866\n",
      "Iteration 384/1000 - Loss: 0.158940\n",
      "Iteration 385/1000 - Loss: 0.145733\n",
      "Iteration 386/1000 - Loss: 0.164893\n",
      "Iteration 387/1000 - Loss: 0.145824\n",
      "Iteration 388/1000 - Loss: 0.183285\n",
      "Iteration 389/1000 - Loss: 0.153558\n",
      "Iteration 390/1000 - Loss: 0.166979\n",
      "Iteration 391/1000 - Loss: 0.158988\n",
      "Iteration 392/1000 - Loss: 0.166206\n",
      "Iteration 393/1000 - Loss: 0.163193\n",
      "Iteration 394/1000 - Loss: 0.161220\n",
      "Iteration 395/1000 - Loss: 0.183989\n",
      "Iteration 396/1000 - Loss: 0.146580\n",
      "Iteration 397/1000 - Loss: 0.149712\n",
      "Iteration 398/1000 - Loss: 0.159401\n",
      "Iteration 399/1000 - Loss: 0.156215\n",
      "Iteration 400/1000 - Loss: 0.150082\n",
      "Iteration 401/1000 - Loss: 0.154455\n",
      "Iteration 402/1000 - Loss: 0.154295\n",
      "Iteration 403/1000 - Loss: 0.150677\n",
      "Iteration 404/1000 - Loss: 0.156720\n",
      "Iteration 405/1000 - Loss: 0.161845\n",
      "Iteration 406/1000 - Loss: 0.154838\n",
      "Iteration 407/1000 - Loss: 0.149863\n",
      "Iteration 408/1000 - Loss: 0.145842\n",
      "Iteration 409/1000 - Loss: 0.164423\n",
      "Iteration 410/1000 - Loss: 0.156428\n",
      "Iteration 411/1000 - Loss: 0.148761\n",
      "Iteration 412/1000 - Loss: 0.182906\n",
      "Iteration 413/1000 - Loss: 0.143279\n",
      "Iteration 414/1000 - Loss: 0.165048\n",
      "Iteration 415/1000 - Loss: 0.151482\n",
      "Iteration 416/1000 - Loss: 0.146572\n",
      "Iteration 417/1000 - Loss: 0.148924\n",
      "Iteration 418/1000 - Loss: 0.151097\n",
      "Iteration 419/1000 - Loss: 0.165443\n",
      "Iteration 420/1000 - Loss: 0.148006\n",
      "Iteration 421/1000 - Loss: 0.144668\n",
      "Iteration 422/1000 - Loss: 0.147183\n",
      "Iteration 423/1000 - Loss: 0.148602\n",
      "Iteration 424/1000 - Loss: 0.161909\n",
      "Iteration 425/1000 - Loss: 0.143099\n",
      "Iteration 426/1000 - Loss: 0.172843\n",
      "Iteration 427/1000 - Loss: 0.151796\n",
      "Iteration 428/1000 - Loss: 0.148481\n",
      "Iteration 429/1000 - Loss: 0.154990\n",
      "Iteration 430/1000 - Loss: 0.159643\n",
      "Iteration 431/1000 - Loss: 0.145758\n",
      "Iteration 432/1000 - Loss: 0.140554\n",
      "Iteration 433/1000 - Loss: 0.144021\n",
      "Iteration 434/1000 - Loss: 0.143502\n",
      "Iteration 435/1000 - Loss: 0.154012\n",
      "Iteration 436/1000 - Loss: 0.144052\n",
      "Iteration 437/1000 - Loss: 0.144426\n",
      "Iteration 438/1000 - Loss: 0.146213\n",
      "Iteration 439/1000 - Loss: 0.145895\n",
      "Iteration 440/1000 - Loss: 0.146464\n",
      "Iteration 441/1000 - Loss: 0.181864\n",
      "Iteration 442/1000 - Loss: 0.147181\n",
      "Iteration 443/1000 - Loss: 0.140332\n",
      "Iteration 444/1000 - Loss: 0.145401\n",
      "Iteration 445/1000 - Loss: 0.142545\n",
      "Iteration 446/1000 - Loss: 0.138588\n",
      "Iteration 447/1000 - Loss: 0.138256\n",
      "Iteration 448/1000 - Loss: 0.161290\n",
      "Iteration 449/1000 - Loss: 0.134926\n",
      "Iteration 450/1000 - Loss: 0.137534\n",
      "Iteration 451/1000 - Loss: 0.157108\n",
      "Iteration 452/1000 - Loss: 0.158456\n",
      "Iteration 453/1000 - Loss: 0.140170\n",
      "Iteration 454/1000 - Loss: 0.134359\n",
      "Iteration 455/1000 - Loss: 0.132519\n",
      "Iteration 456/1000 - Loss: 0.133637\n",
      "Iteration 457/1000 - Loss: 0.139531\n",
      "Iteration 458/1000 - Loss: 0.141430\n",
      "Iteration 459/1000 - Loss: 0.133914\n",
      "Iteration 460/1000 - Loss: 0.136155\n",
      "Iteration 461/1000 - Loss: 0.140837\n",
      "Iteration 462/1000 - Loss: 0.145025\n",
      "Iteration 463/1000 - Loss: 0.145587\n",
      "Iteration 464/1000 - Loss: 0.142985\n",
      "Iteration 465/1000 - Loss: 0.143958\n",
      "Iteration 466/1000 - Loss: 0.127470\n",
      "Iteration 467/1000 - Loss: 0.130549\n",
      "Iteration 468/1000 - Loss: 0.139150\n",
      "Iteration 469/1000 - Loss: 0.135961\n",
      "Iteration 470/1000 - Loss: 0.160717\n",
      "Iteration 471/1000 - Loss: 0.141505\n",
      "Iteration 472/1000 - Loss: 0.136539\n",
      "Iteration 473/1000 - Loss: 0.144265\n",
      "Iteration 474/1000 - Loss: 0.133658\n",
      "Iteration 475/1000 - Loss: 0.138758\n",
      "Iteration 476/1000 - Loss: 0.138485\n",
      "Iteration 477/1000 - Loss: 0.140170\n",
      "Iteration 478/1000 - Loss: 0.130474\n",
      "Iteration 479/1000 - Loss: 0.135829\n",
      "Iteration 480/1000 - Loss: 0.135322\n",
      "Iteration 481/1000 - Loss: 0.168274\n",
      "Iteration 482/1000 - Loss: 0.127681\n",
      "Iteration 483/1000 - Loss: 0.157291\n",
      "Iteration 484/1000 - Loss: 0.133964\n",
      "Iteration 485/1000 - Loss: 0.124787\n",
      "Iteration 486/1000 - Loss: 0.173445\n",
      "Iteration 487/1000 - Loss: 0.141394\n",
      "Iteration 488/1000 - Loss: 0.129234\n",
      "Iteration 489/1000 - Loss: 0.129954\n",
      "Iteration 490/1000 - Loss: 0.126534\n",
      "Iteration 491/1000 - Loss: 0.145400\n",
      "Iteration 492/1000 - Loss: 0.136325\n",
      "Iteration 493/1000 - Loss: 0.128943\n",
      "Iteration 494/1000 - Loss: 0.128941\n",
      "Iteration 495/1000 - Loss: 0.140403\n",
      "Iteration 496/1000 - Loss: 0.131014\n",
      "Iteration 497/1000 - Loss: 0.129248\n",
      "Iteration 498/1000 - Loss: 0.125944\n",
      "Iteration 499/1000 - Loss: 0.123806\n",
      "Iteration 500/1000 - Loss: 0.126103\n",
      "Iteration 501/1000 - Loss: 0.140229\n",
      "Iteration 502/1000 - Loss: 0.127212\n",
      "Iteration 503/1000 - Loss: 0.144287\n",
      "Iteration 504/1000 - Loss: 0.156265\n",
      "Iteration 505/1000 - Loss: 0.132652\n",
      "Iteration 506/1000 - Loss: 0.129799\n",
      "Iteration 507/1000 - Loss: 0.162027\n",
      "Iteration 508/1000 - Loss: 0.126391\n",
      "Iteration 509/1000 - Loss: 0.135484\n",
      "Iteration 510/1000 - Loss: 0.131062\n",
      "Iteration 511/1000 - Loss: 0.128851\n",
      "Iteration 512/1000 - Loss: 0.131475\n",
      "Iteration 513/1000 - Loss: 0.128875\n",
      "Iteration 514/1000 - Loss: 0.143130\n",
      "Iteration 515/1000 - Loss: 0.128586\n",
      "Iteration 516/1000 - Loss: 0.123429\n",
      "Iteration 517/1000 - Loss: 0.145861\n",
      "Iteration 518/1000 - Loss: 0.135036\n",
      "Iteration 519/1000 - Loss: 0.133041\n",
      "Iteration 520/1000 - Loss: 0.122445\n",
      "Iteration 521/1000 - Loss: 0.134855\n",
      "Iteration 522/1000 - Loss: 0.127196\n",
      "Iteration 523/1000 - Loss: 0.147627\n",
      "Iteration 524/1000 - Loss: 0.123449\n",
      "Iteration 525/1000 - Loss: 0.144264\n",
      "Iteration 526/1000 - Loss: 0.133331\n",
      "Iteration 527/1000 - Loss: 0.129359\n",
      "Iteration 528/1000 - Loss: 0.126326\n",
      "Iteration 529/1000 - Loss: 0.149984\n",
      "Iteration 530/1000 - Loss: 0.118781\n",
      "Iteration 531/1000 - Loss: 0.122443\n",
      "Iteration 532/1000 - Loss: 0.126076\n",
      "Iteration 533/1000 - Loss: 0.117487\n",
      "Iteration 534/1000 - Loss: 0.125662\n",
      "Iteration 535/1000 - Loss: 0.120390\n",
      "Iteration 536/1000 - Loss: 0.118780\n",
      "Iteration 537/1000 - Loss: 0.125593\n",
      "Iteration 538/1000 - Loss: 0.119088\n",
      "Iteration 539/1000 - Loss: 0.127026\n",
      "Iteration 540/1000 - Loss: 0.122740\n",
      "Iteration 541/1000 - Loss: 0.123379\n",
      "Iteration 542/1000 - Loss: 0.131958\n",
      "Iteration 543/1000 - Loss: 0.143300\n",
      "Iteration 544/1000 - Loss: 0.128816\n",
      "Iteration 545/1000 - Loss: 0.127991\n",
      "Iteration 546/1000 - Loss: 0.125958\n",
      "Iteration 547/1000 - Loss: 0.125438\n",
      "Iteration 548/1000 - Loss: 0.123327\n",
      "Iteration 549/1000 - Loss: 0.121913\n",
      "Iteration 550/1000 - Loss: 0.129791\n",
      "Iteration 551/1000 - Loss: 0.120792\n",
      "Iteration 552/1000 - Loss: 0.125686\n",
      "Iteration 553/1000 - Loss: 0.121449\n",
      "Iteration 554/1000 - Loss: 0.143534\n",
      "Iteration 555/1000 - Loss: 0.126949\n",
      "Iteration 556/1000 - Loss: 0.126913\n",
      "Iteration 557/1000 - Loss: 0.131368\n",
      "Iteration 558/1000 - Loss: 0.126635\n",
      "Iteration 559/1000 - Loss: 0.124710\n",
      "Iteration 560/1000 - Loss: 0.120829\n",
      "Iteration 561/1000 - Loss: 0.117156\n",
      "Iteration 562/1000 - Loss: 0.115332\n",
      "Iteration 563/1000 - Loss: 0.122776\n",
      "Iteration 564/1000 - Loss: 0.115476\n",
      "Iteration 565/1000 - Loss: 0.125364\n",
      "Iteration 566/1000 - Loss: 0.114789\n",
      "Iteration 567/1000 - Loss: 0.116850\n",
      "Iteration 568/1000 - Loss: 0.159412\n",
      "Iteration 569/1000 - Loss: 0.126338\n",
      "Iteration 570/1000 - Loss: 0.115855\n",
      "Iteration 571/1000 - Loss: 0.120358\n",
      "Iteration 572/1000 - Loss: 0.133748\n",
      "Iteration 573/1000 - Loss: 0.121424\n",
      "Iteration 574/1000 - Loss: 0.120163\n",
      "Iteration 575/1000 - Loss: 0.131102\n",
      "Iteration 576/1000 - Loss: 0.116573\n",
      "Iteration 577/1000 - Loss: 0.113817\n",
      "Iteration 578/1000 - Loss: 0.150278\n",
      "Iteration 579/1000 - Loss: 0.115453\n",
      "Iteration 580/1000 - Loss: 0.117619\n",
      "Iteration 581/1000 - Loss: 0.147289\n",
      "Iteration 582/1000 - Loss: 0.125343\n",
      "Iteration 583/1000 - Loss: 0.148438\n",
      "Iteration 584/1000 - Loss: 0.130264\n",
      "Iteration 585/1000 - Loss: 0.120651\n",
      "Iteration 586/1000 - Loss: 0.115223\n",
      "Iteration 587/1000 - Loss: 0.126026\n",
      "Iteration 588/1000 - Loss: 0.116117\n",
      "Iteration 589/1000 - Loss: 0.113183\n",
      "Iteration 590/1000 - Loss: 0.108836\n",
      "Iteration 591/1000 - Loss: 0.123064\n",
      "Iteration 592/1000 - Loss: 0.111067\n",
      "Iteration 593/1000 - Loss: 0.121930\n",
      "Iteration 594/1000 - Loss: 0.111960\n",
      "Iteration 595/1000 - Loss: 0.113830\n",
      "Iteration 596/1000 - Loss: 0.117455\n",
      "Iteration 597/1000 - Loss: 0.114627\n",
      "Iteration 598/1000 - Loss: 0.123426\n",
      "Iteration 599/1000 - Loss: 0.124430\n",
      "Iteration 600/1000 - Loss: 0.110692\n",
      "Iteration 601/1000 - Loss: 0.113921\n",
      "Iteration 602/1000 - Loss: 0.111247\n",
      "Iteration 603/1000 - Loss: 0.120798\n",
      "Iteration 604/1000 - Loss: 0.137420\n",
      "Iteration 605/1000 - Loss: 0.123712\n",
      "Iteration 606/1000 - Loss: 0.112601\n",
      "Iteration 607/1000 - Loss: 0.109491\n",
      "Iteration 608/1000 - Loss: 0.114663\n",
      "Iteration 609/1000 - Loss: 0.113504\n",
      "Iteration 610/1000 - Loss: 0.121184\n",
      "Iteration 611/1000 - Loss: 0.116129\n",
      "Iteration 612/1000 - Loss: 0.116948\n",
      "Iteration 613/1000 - Loss: 0.110891\n",
      "Iteration 614/1000 - Loss: 0.107095\n",
      "Iteration 615/1000 - Loss: 0.110995\n",
      "Iteration 616/1000 - Loss: 0.108278\n",
      "Iteration 617/1000 - Loss: 0.146521\n",
      "Iteration 618/1000 - Loss: 0.104765\n",
      "Iteration 619/1000 - Loss: 0.115491\n",
      "Iteration 620/1000 - Loss: 0.111494\n",
      "Iteration 621/1000 - Loss: 0.114693\n",
      "Iteration 622/1000 - Loss: 0.115432\n",
      "Iteration 623/1000 - Loss: 0.125356\n",
      "Iteration 624/1000 - Loss: 0.143843\n",
      "Iteration 625/1000 - Loss: 0.142662\n",
      "Iteration 626/1000 - Loss: 0.108912\n",
      "Iteration 627/1000 - Loss: 0.119377\n",
      "Iteration 628/1000 - Loss: 0.109805\n",
      "Iteration 629/1000 - Loss: 0.116875\n",
      "Iteration 630/1000 - Loss: 0.122849\n",
      "Iteration 631/1000 - Loss: 0.105762\n",
      "Iteration 632/1000 - Loss: 0.109962\n",
      "Iteration 633/1000 - Loss: 0.110013\n",
      "Iteration 634/1000 - Loss: 0.104842\n",
      "Iteration 635/1000 - Loss: 0.106170\n",
      "Iteration 636/1000 - Loss: 0.104518\n",
      "Iteration 637/1000 - Loss: 0.149989\n",
      "Iteration 638/1000 - Loss: 0.105067\n",
      "Iteration 639/1000 - Loss: 0.111181\n",
      "Iteration 640/1000 - Loss: 0.107714\n",
      "Iteration 641/1000 - Loss: 0.114058\n",
      "Iteration 642/1000 - Loss: 0.102983\n",
      "Iteration 643/1000 - Loss: 0.103647\n",
      "Iteration 644/1000 - Loss: 0.113962\n",
      "Iteration 645/1000 - Loss: 0.145749\n",
      "Iteration 646/1000 - Loss: 0.101890\n",
      "Iteration 647/1000 - Loss: 0.108850\n",
      "Iteration 648/1000 - Loss: 0.124314\n",
      "Iteration 649/1000 - Loss: 0.104352\n",
      "Iteration 650/1000 - Loss: 0.119361\n",
      "Iteration 651/1000 - Loss: 0.113127\n",
      "Iteration 652/1000 - Loss: 0.106640\n",
      "Iteration 653/1000 - Loss: 0.099242\n",
      "Iteration 654/1000 - Loss: 0.109548\n",
      "Iteration 655/1000 - Loss: 0.102940\n",
      "Iteration 656/1000 - Loss: 0.118223\n",
      "Iteration 657/1000 - Loss: 0.137063\n",
      "Iteration 658/1000 - Loss: 0.107375\n",
      "Iteration 659/1000 - Loss: 0.108898\n",
      "Iteration 660/1000 - Loss: 0.128729\n",
      "Iteration 661/1000 - Loss: 0.105607\n",
      "Iteration 662/1000 - Loss: 0.099912\n",
      "Iteration 663/1000 - Loss: 0.112208\n",
      "Iteration 664/1000 - Loss: 0.099106\n",
      "Iteration 665/1000 - Loss: 0.100022\n",
      "Iteration 666/1000 - Loss: 0.111137\n",
      "Iteration 667/1000 - Loss: 0.116170\n",
      "Iteration 668/1000 - Loss: 0.102844\n",
      "Iteration 669/1000 - Loss: 0.104547\n",
      "Iteration 670/1000 - Loss: 0.103689\n",
      "Iteration 671/1000 - Loss: 0.097976\n",
      "Iteration 672/1000 - Loss: 0.112823\n",
      "Iteration 673/1000 - Loss: 0.115168\n",
      "Iteration 674/1000 - Loss: 0.104349\n",
      "Iteration 675/1000 - Loss: 0.103669\n",
      "Iteration 676/1000 - Loss: 0.110961\n",
      "Iteration 677/1000 - Loss: 0.104575\n",
      "Iteration 678/1000 - Loss: 0.104071\n",
      "Iteration 679/1000 - Loss: 0.101069\n",
      "Iteration 680/1000 - Loss: 0.104370\n",
      "Iteration 681/1000 - Loss: 0.129586\n",
      "Iteration 682/1000 - Loss: 0.122184\n",
      "Iteration 683/1000 - Loss: 0.122571\n",
      "Iteration 684/1000 - Loss: 0.097662\n",
      "Iteration 685/1000 - Loss: 0.099521\n",
      "Iteration 686/1000 - Loss: 0.100579\n",
      "Iteration 687/1000 - Loss: 0.103074\n",
      "Iteration 688/1000 - Loss: 0.113030\n",
      "Iteration 689/1000 - Loss: 0.099003\n",
      "Iteration 690/1000 - Loss: 0.100518\n",
      "Iteration 691/1000 - Loss: 0.100055\n",
      "Iteration 692/1000 - Loss: 0.106992\n",
      "Iteration 693/1000 - Loss: 0.109338\n",
      "Iteration 694/1000 - Loss: 0.109652\n",
      "Iteration 695/1000 - Loss: 0.110333\n",
      "Iteration 696/1000 - Loss: 0.128049\n",
      "Iteration 697/1000 - Loss: 0.095343\n",
      "Iteration 698/1000 - Loss: 0.097680\n",
      "Iteration 699/1000 - Loss: 0.103538\n",
      "Iteration 700/1000 - Loss: 0.102882\n",
      "Iteration 701/1000 - Loss: 0.100712\n",
      "Iteration 702/1000 - Loss: 0.121445\n",
      "Iteration 703/1000 - Loss: 0.099928\n",
      "Iteration 704/1000 - Loss: 0.096717\n",
      "Iteration 705/1000 - Loss: 0.094056\n",
      "Iteration 706/1000 - Loss: 0.097345\n",
      "Iteration 707/1000 - Loss: 0.102318\n",
      "Iteration 708/1000 - Loss: 0.103946\n",
      "Iteration 709/1000 - Loss: 0.099099\n",
      "Iteration 710/1000 - Loss: 0.096253\n",
      "Iteration 711/1000 - Loss: 0.121518\n",
      "Iteration 712/1000 - Loss: 0.098730\n",
      "Iteration 713/1000 - Loss: 0.125402\n",
      "Iteration 714/1000 - Loss: 0.101806\n",
      "Iteration 715/1000 - Loss: 0.100626\n",
      "Iteration 716/1000 - Loss: 0.096180\n",
      "Iteration 717/1000 - Loss: 0.103361\n",
      "Iteration 718/1000 - Loss: 0.096308\n",
      "Iteration 719/1000 - Loss: 0.115205\n",
      "Iteration 720/1000 - Loss: 0.099162\n",
      "Iteration 721/1000 - Loss: 0.101278\n",
      "Iteration 722/1000 - Loss: 0.119416\n",
      "Iteration 723/1000 - Loss: 0.120473\n",
      "Iteration 724/1000 - Loss: 0.096356\n",
      "Iteration 725/1000 - Loss: 0.099138\n",
      "Iteration 726/1000 - Loss: 0.093876\n",
      "Iteration 727/1000 - Loss: 0.106669\n",
      "Iteration 728/1000 - Loss: 0.095552\n",
      "Iteration 729/1000 - Loss: 0.098619\n",
      "Iteration 730/1000 - Loss: 0.098597\n",
      "Iteration 731/1000 - Loss: 0.093953\n",
      "Iteration 732/1000 - Loss: 0.118538\n",
      "Iteration 733/1000 - Loss: 0.096707\n",
      "Iteration 734/1000 - Loss: 0.090758\n",
      "Iteration 735/1000 - Loss: 0.092720\n",
      "Iteration 736/1000 - Loss: 0.091114\n",
      "Iteration 737/1000 - Loss: 0.116663\n",
      "Iteration 738/1000 - Loss: 0.090181\n",
      "Iteration 739/1000 - Loss: 0.117186\n",
      "Iteration 740/1000 - Loss: 0.091710\n",
      "Iteration 741/1000 - Loss: 0.095149\n",
      "Iteration 742/1000 - Loss: 0.099189\n",
      "Iteration 743/1000 - Loss: 0.099238\n",
      "Iteration 744/1000 - Loss: 0.091620\n",
      "Iteration 745/1000 - Loss: 0.091597\n",
      "Iteration 746/1000 - Loss: 0.105628\n",
      "Iteration 747/1000 - Loss: 0.094829\n",
      "Iteration 748/1000 - Loss: 0.091678\n",
      "Iteration 749/1000 - Loss: 0.095952\n",
      "Iteration 750/1000 - Loss: 0.106773\n",
      "Iteration 751/1000 - Loss: 0.097877\n",
      "Iteration 752/1000 - Loss: 0.090494\n",
      "Iteration 753/1000 - Loss: 0.093415\n",
      "Iteration 754/1000 - Loss: 0.102826\n",
      "Iteration 755/1000 - Loss: 0.095390\n",
      "Iteration 756/1000 - Loss: 0.091684\n",
      "Iteration 757/1000 - Loss: 0.100516\n",
      "Iteration 758/1000 - Loss: 0.097299\n",
      "Iteration 759/1000 - Loss: 0.098504\n",
      "Iteration 760/1000 - Loss: 0.103102\n",
      "Iteration 761/1000 - Loss: 0.088571\n",
      "Iteration 762/1000 - Loss: 0.099329\n",
      "Iteration 763/1000 - Loss: 0.096152\n",
      "Iteration 764/1000 - Loss: 0.088225\n",
      "Iteration 765/1000 - Loss: 0.097701\n",
      "Iteration 766/1000 - Loss: 0.091495\n",
      "Iteration 767/1000 - Loss: 0.114706\n",
      "Iteration 768/1000 - Loss: 0.095180\n",
      "Iteration 769/1000 - Loss: 0.094708\n",
      "Iteration 770/1000 - Loss: 0.094223\n",
      "Iteration 771/1000 - Loss: 0.088040\n",
      "Iteration 772/1000 - Loss: 0.092587\n",
      "Iteration 773/1000 - Loss: 0.090305\n",
      "Iteration 774/1000 - Loss: 0.094836\n",
      "Iteration 775/1000 - Loss: 0.097544\n",
      "Iteration 776/1000 - Loss: 0.124395\n",
      "Iteration 777/1000 - Loss: 0.101100\n",
      "Iteration 778/1000 - Loss: 0.092432\n",
      "Iteration 779/1000 - Loss: 0.092730\n",
      "Iteration 780/1000 - Loss: 0.092822\n",
      "Iteration 781/1000 - Loss: 0.089950\n",
      "Iteration 782/1000 - Loss: 0.086856\n",
      "Iteration 783/1000 - Loss: 0.098156\n",
      "Iteration 784/1000 - Loss: 0.087966\n",
      "Iteration 785/1000 - Loss: 0.091929\n",
      "Iteration 786/1000 - Loss: 0.087320\n",
      "Iteration 787/1000 - Loss: 0.091686\n",
      "Iteration 788/1000 - Loss: 0.090183\n",
      "Iteration 789/1000 - Loss: 0.102209\n",
      "Iteration 790/1000 - Loss: 0.095641\n",
      "Iteration 791/1000 - Loss: 0.085018\n",
      "Iteration 792/1000 - Loss: 0.088910\n",
      "Iteration 793/1000 - Loss: 0.085256\n",
      "Iteration 794/1000 - Loss: 0.094464\n",
      "Iteration 795/1000 - Loss: 0.100003\n",
      "Iteration 796/1000 - Loss: 0.084937\n",
      "Iteration 797/1000 - Loss: 0.088540\n",
      "Iteration 798/1000 - Loss: 0.091415\n",
      "Iteration 799/1000 - Loss: 0.089082\n",
      "Iteration 800/1000 - Loss: 0.084336\n",
      "Iteration 801/1000 - Loss: 0.102962\n",
      "Iteration 802/1000 - Loss: 0.092593\n",
      "Iteration 803/1000 - Loss: 0.087965\n",
      "Iteration 804/1000 - Loss: 0.088290\n",
      "Iteration 805/1000 - Loss: 0.090774\n",
      "Iteration 806/1000 - Loss: 0.085568\n",
      "Iteration 807/1000 - Loss: 0.086542\n",
      "Iteration 808/1000 - Loss: 0.083552\n",
      "Iteration 809/1000 - Loss: 0.090572\n",
      "Iteration 810/1000 - Loss: 0.084998\n",
      "Iteration 811/1000 - Loss: 0.083404\n",
      "Iteration 812/1000 - Loss: 0.085283\n",
      "Iteration 813/1000 - Loss: 0.087750\n",
      "Iteration 814/1000 - Loss: 0.085731\n",
      "Iteration 815/1000 - Loss: 0.090133\n",
      "Iteration 816/1000 - Loss: 0.092361\n",
      "Iteration 817/1000 - Loss: 0.094160\n",
      "Iteration 818/1000 - Loss: 0.100023\n",
      "Iteration 819/1000 - Loss: 0.098492\n",
      "Iteration 820/1000 - Loss: 0.085385\n",
      "Iteration 821/1000 - Loss: 0.113242\n",
      "Iteration 822/1000 - Loss: 0.090503\n",
      "Iteration 823/1000 - Loss: 0.085383\n",
      "Iteration 824/1000 - Loss: 0.090290\n",
      "Iteration 825/1000 - Loss: 0.081755\n",
      "Iteration 826/1000 - Loss: 0.083990\n",
      "Iteration 827/1000 - Loss: 0.087149\n",
      "Iteration 828/1000 - Loss: 0.110146\n",
      "Iteration 829/1000 - Loss: 0.082773\n",
      "Iteration 830/1000 - Loss: 0.109252\n",
      "Iteration 831/1000 - Loss: 0.117661\n",
      "Iteration 832/1000 - Loss: 0.087362\n",
      "Iteration 833/1000 - Loss: 0.087989\n",
      "Iteration 834/1000 - Loss: 0.081605\n",
      "Iteration 835/1000 - Loss: 0.087000\n",
      "Iteration 836/1000 - Loss: 0.083164\n",
      "Iteration 837/1000 - Loss: 0.081892\n",
      "Iteration 838/1000 - Loss: 0.088167\n",
      "Iteration 839/1000 - Loss: 0.083389\n",
      "Iteration 840/1000 - Loss: 0.089297\n",
      "Iteration 841/1000 - Loss: 0.081880\n",
      "Iteration 842/1000 - Loss: 0.094628\n",
      "Iteration 843/1000 - Loss: 0.091794\n",
      "Iteration 844/1000 - Loss: 0.119996\n",
      "Iteration 845/1000 - Loss: 0.081365\n",
      "Iteration 846/1000 - Loss: 0.085047\n",
      "Iteration 847/1000 - Loss: 0.089148\n",
      "Iteration 848/1000 - Loss: 0.096103\n",
      "Iteration 849/1000 - Loss: 0.083597\n",
      "Iteration 850/1000 - Loss: 0.087373\n",
      "Iteration 851/1000 - Loss: 0.087720\n",
      "Iteration 852/1000 - Loss: 0.082306\n",
      "Iteration 853/1000 - Loss: 0.108060\n",
      "Iteration 854/1000 - Loss: 0.084022\n",
      "Iteration 855/1000 - Loss: 0.083577\n",
      "Iteration 856/1000 - Loss: 0.082815\n",
      "Iteration 857/1000 - Loss: 0.088243\n",
      "Iteration 858/1000 - Loss: 0.080010\n",
      "Iteration 859/1000 - Loss: 0.080786\n",
      "Iteration 860/1000 - Loss: 0.079605\n",
      "Iteration 861/1000 - Loss: 0.085937\n",
      "Iteration 862/1000 - Loss: 0.088459\n",
      "Iteration 863/1000 - Loss: 0.085657\n",
      "Iteration 864/1000 - Loss: 0.087891\n",
      "Iteration 865/1000 - Loss: 0.093017\n",
      "Iteration 866/1000 - Loss: 0.085068\n",
      "Iteration 867/1000 - Loss: 0.090645\n",
      "Iteration 868/1000 - Loss: 0.089376\n",
      "Iteration 869/1000 - Loss: 0.085621\n",
      "Iteration 870/1000 - Loss: 0.091377\n",
      "Iteration 871/1000 - Loss: 0.078681\n",
      "Iteration 872/1000 - Loss: 0.083514\n",
      "Iteration 873/1000 - Loss: 0.084039\n",
      "Iteration 874/1000 - Loss: 0.086898\n",
      "Iteration 875/1000 - Loss: 0.082459\n",
      "Iteration 876/1000 - Loss: 0.079527\n",
      "Iteration 877/1000 - Loss: 0.082476\n",
      "Iteration 878/1000 - Loss: 0.080052\n",
      "Iteration 879/1000 - Loss: 0.083225\n",
      "Iteration 880/1000 - Loss: 0.080524\n",
      "Iteration 881/1000 - Loss: 0.080088\n",
      "Iteration 882/1000 - Loss: 0.081248\n",
      "Iteration 883/1000 - Loss: 0.085709\n",
      "Iteration 884/1000 - Loss: 0.080079\n",
      "Iteration 885/1000 - Loss: 0.085616\n",
      "Iteration 886/1000 - Loss: 0.084076\n",
      "Iteration 887/1000 - Loss: 0.077494\n",
      "Iteration 888/1000 - Loss: 0.093861\n",
      "Iteration 889/1000 - Loss: 0.079643\n",
      "Iteration 890/1000 - Loss: 0.078908\n",
      "Iteration 891/1000 - Loss: 0.086720\n",
      "Iteration 892/1000 - Loss: 0.097317\n",
      "Iteration 893/1000 - Loss: 0.081759\n",
      "Iteration 894/1000 - Loss: 0.084037\n",
      "Iteration 895/1000 - Loss: 0.079291\n",
      "Iteration 896/1000 - Loss: 0.088816\n",
      "Iteration 897/1000 - Loss: 0.090753\n",
      "Iteration 898/1000 - Loss: 0.082985\n",
      "Iteration 899/1000 - Loss: 0.078263\n",
      "Iteration 900/1000 - Loss: 0.083921\n",
      "Iteration 901/1000 - Loss: 0.087708\n",
      "Iteration 902/1000 - Loss: 0.089704\n",
      "Iteration 903/1000 - Loss: 0.091932\n",
      "Iteration 904/1000 - Loss: 0.085581\n",
      "Iteration 905/1000 - Loss: 0.079907\n",
      "Iteration 906/1000 - Loss: 0.076426\n",
      "Iteration 907/1000 - Loss: 0.083545\n",
      "Iteration 908/1000 - Loss: 0.076391\n",
      "Iteration 909/1000 - Loss: 0.079895\n",
      "Iteration 910/1000 - Loss: 0.088786\n",
      "Iteration 911/1000 - Loss: 0.078262\n",
      "Iteration 912/1000 - Loss: 0.076961\n",
      "Iteration 913/1000 - Loss: 0.077101\n",
      "Iteration 914/1000 - Loss: 0.082123\n",
      "Iteration 915/1000 - Loss: 0.087004\n",
      "Iteration 916/1000 - Loss: 0.083829\n",
      "Iteration 917/1000 - Loss: 0.099570\n",
      "Iteration 918/1000 - Loss: 0.084863\n",
      "Iteration 919/1000 - Loss: 0.081549\n",
      "Iteration 920/1000 - Loss: 0.079373\n",
      "Iteration 921/1000 - Loss: 0.080465\n",
      "Iteration 922/1000 - Loss: 0.079909\n",
      "Iteration 923/1000 - Loss: 0.076044\n",
      "Iteration 924/1000 - Loss: 0.082949\n",
      "Iteration 925/1000 - Loss: 0.083078\n",
      "Iteration 926/1000 - Loss: 0.104544\n",
      "Iteration 927/1000 - Loss: 0.075974\n",
      "Iteration 928/1000 - Loss: 0.089204\n",
      "Iteration 929/1000 - Loss: 0.077415\n",
      "Iteration 930/1000 - Loss: 0.081306\n",
      "Iteration 931/1000 - Loss: 0.074081\n",
      "Iteration 932/1000 - Loss: 0.077494\n",
      "Iteration 933/1000 - Loss: 0.085983\n",
      "Iteration 934/1000 - Loss: 0.073696\n",
      "Iteration 935/1000 - Loss: 0.075505\n",
      "Iteration 936/1000 - Loss: 0.074266\n",
      "Iteration 937/1000 - Loss: 0.098801\n",
      "Iteration 938/1000 - Loss: 0.077559\n",
      "Iteration 939/1000 - Loss: 0.083148\n",
      "Iteration 940/1000 - Loss: 0.081046\n",
      "Iteration 941/1000 - Loss: 0.078585\n",
      "Iteration 942/1000 - Loss: 0.082979\n",
      "Iteration 943/1000 - Loss: 0.077172\n",
      "Iteration 944/1000 - Loss: 0.076989\n",
      "Iteration 945/1000 - Loss: 0.081128\n",
      "Iteration 946/1000 - Loss: 0.098694\n",
      "Iteration 947/1000 - Loss: 0.084515\n",
      "Iteration 948/1000 - Loss: 0.098291\n",
      "Iteration 949/1000 - Loss: 0.103822\n",
      "Iteration 950/1000 - Loss: 0.074934\n",
      "Iteration 951/1000 - Loss: 0.075652\n",
      "Iteration 952/1000 - Loss: 0.090257\n",
      "Iteration 953/1000 - Loss: 0.073877\n",
      "Iteration 954/1000 - Loss: 0.083450\n",
      "Iteration 955/1000 - Loss: 0.081085\n",
      "Iteration 956/1000 - Loss: 0.094162\n",
      "Iteration 957/1000 - Loss: 0.077623\n",
      "Iteration 958/1000 - Loss: 0.073075\n",
      "Iteration 959/1000 - Loss: 0.079928\n",
      "Iteration 960/1000 - Loss: 0.078932\n",
      "Iteration 961/1000 - Loss: 0.098177\n",
      "Iteration 962/1000 - Loss: 0.075533\n",
      "Iteration 963/1000 - Loss: 0.074998\n",
      "Iteration 964/1000 - Loss: 0.076422\n",
      "Iteration 965/1000 - Loss: 0.075449\n",
      "Iteration 966/1000 - Loss: 0.085597\n",
      "Iteration 967/1000 - Loss: 0.073825\n",
      "Iteration 968/1000 - Loss: 0.082368\n",
      "Iteration 969/1000 - Loss: 0.084718\n",
      "Iteration 970/1000 - Loss: 0.071191\n",
      "Iteration 971/1000 - Loss: 0.077339\n",
      "Iteration 972/1000 - Loss: 0.075573\n",
      "Iteration 973/1000 - Loss: 0.075312\n",
      "Iteration 974/1000 - Loss: 0.082003\n",
      "Iteration 975/1000 - Loss: 0.073326\n",
      "Iteration 976/1000 - Loss: 0.071844\n",
      "Iteration 977/1000 - Loss: 0.081398\n",
      "Iteration 978/1000 - Loss: 0.075316\n",
      "Iteration 979/1000 - Loss: 0.083121\n",
      "Iteration 980/1000 - Loss: 0.072953\n",
      "Iteration 981/1000 - Loss: 0.082755\n",
      "Iteration 982/1000 - Loss: 0.071513\n",
      "Iteration 983/1000 - Loss: 0.072721\n",
      "Iteration 984/1000 - Loss: 0.077122\n",
      "Iteration 985/1000 - Loss: 0.075142\n",
      "Iteration 986/1000 - Loss: 0.074039\n",
      "Iteration 987/1000 - Loss: 0.075625\n",
      "Iteration 988/1000 - Loss: 0.074666\n",
      "Iteration 989/1000 - Loss: 0.080491\n",
      "Iteration 990/1000 - Loss: 0.081026\n",
      "Iteration 991/1000 - Loss: 0.079177\n",
      "Iteration 992/1000 - Loss: 0.075245\n",
      "Iteration 993/1000 - Loss: 0.071545\n",
      "Iteration 994/1000 - Loss: 0.071683\n",
      "Iteration 995/1000 - Loss: 0.078016\n",
      "Iteration 996/1000 - Loss: 0.073938\n",
      "Iteration 997/1000 - Loss: 0.076176\n",
      "Iteration 998/1000 - Loss: 0.073344\n",
      "Iteration 999/1000 - Loss: 0.076915\n",
      "Iteration 1000/1000 - Loss: 0.070626\n",
      "Accuracy of the Adaline classifier: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Create synthetic binary classification data\n",
    "X, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=42)\n",
    "\n",
    "# Split the dataset into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Instantiate the Adaline classifier\n",
    "adaline = Adaline(learning_rate=0.01, max_iter=1000, batch_size=16, tol=1e-4)\n",
    "\n",
    "# Train the Adaline classifier\n",
    "adaline.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels on the test set\n",
    "y_pred = adaline.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of the Adaline classifier: {accuracy * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
